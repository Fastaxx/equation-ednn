Les PINN ou EDNN coupl√©s avec ReLU et d'autres fonctions d'activation non lisses, telles que ELU et SELU (Exponential and Scaled Exponential Linear Units) ne sont pas des m√©thodes "convergentes" : pour un ensemble de donn√©es d'apprentissage infini, un PINN / EDNN bien entra√Æn√©e avec des fonctions d'activation de type ReLU ne converge pas vers la solution exacte [ref]. Ce r√©sultat th√©orique est √©galement confirm√© pa exp√©riences utilisant des fonctions d'activation de type ReLU.  Pas de fonctions d'activation de type ReLU dans les PINN/EDNN

Relu pas bon : 
Remark 2.5. Algorithm 2.3 requires the residual (2.12), for the neural network uùúÉ , to be evaluated pointwise for every training step. Hence, one needs the neural network to be sufficiently regular. Depending on the order of derivatives in D of (2.1), this can be ensured by requiring sufficient smoothness for the activation function. Hence, the ReLU activation function, which is only Lipschitz continuous, might not be admissible in this framework. On the other hand, smooth activation functions such as logistic and tanh are always admissible - https://arxiv.org/pdf/2006.16144.pdf